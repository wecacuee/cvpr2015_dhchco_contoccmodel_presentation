
\paragraph{Occlusion handling in detection}
Several works in object detection consider occlusion by training a detector on visible parts of the object \cite{Gao_etal_2011}. Occlusion reasoning based on 2D image silhouettes is used to improve detection performance in \cite{Hsiao_Herbert_2012}. In contrast, our occlusion reasoning is based on 3D entities. In recent years, object detectors have also considered occlusion reasoning using 3D cues, often learned from a dataset of CAD models \cite{Pepik_etal_2012,Pepik_etal_2013,Xiang_Savarese_2013}. By necessity, such frameworks are often a discrete representation of occlusion behavior, for example, in the form of a collection of occlusion masks derived from object configurations discretized over viewpoint. In constrast to these works, our occlusion modeling is also fully 3D, but allows for a continuous representation. Further, to derive 3D information, we do not use CAD models, rather we derive a probabilistic formulation based on physical insights.

\paragraph{Occlusion handling in tracking}
Occlusions have also been handled in tracking-by-detection frameworks by considering occluder patterns in the image \cite{Kwak_etal_2012,Wu_Nevatia_2007}. A notable exception is the work of Milan et al.~\cite{Milan_etal_2014} that explicits models occlusion in the continuous domain to determine a visibility for each object in multi-target tracking. However, the occlusion model in \cite{Milan_etal_2014} is essentially the overlap of image projections of a Gaussian representation of the object. Our occlusion modeling is much more general in determining the probability of a point in space as belonging to an object. While it can also be used to determine a visibility ratio similar to \cite{Milan_etal_2014}, it can have far more general applications and can be quantitatively evaluated, as shown by our experiments on point track associations.

\paragraph{3D localization and scene understanding}
One of the central goals of 3D scene understanding is to localize the 3D positions and orientations of objects in complex scenes. For instance, using stereo imagery, several visual cues are combined in \cite{Geiger_etal_2014} to simultaneously determine object locations and a rough intersection topology. Similar to us, other works have also considered monocular frameworks. Notably, occlusions are explicitly handled in \cite{Wojek_etal_2013} by considering partial object detectors. A detailed part-based representation of objects based on annotated CAD models is used for monocular scene understanding in \cite{Zia_etal_2013,Zia_etal_2014}, which also allows reasoning about mutual occlusions between objects. In contrast to these works, our monocular framework uses a physical modeling of occlusion in continuous space, which makes it more general, extensible and amenable for continuous optimization.

\paragraph{Motion segmentation and multibody SFM}
An application for our occlusion modeling is to determine point track associations in scenes with multiple objects. For moving objects, this is within the purview of motion segmentation, which has been actively studied \cite{Tron_Vidal_2007,Rao_etal_2010}. This also motivates further applications such as object segmentation based on point trajectories \cite{Brox_Malik_2010}. Motion segmentation is also used within multibody structure from motion (SFM) frameworks \cite{Ozden_etal_2010,Kundu_etal_2011,Namdev2012}. In contrast to these works, our formulation does not distinguish between moving and static objects and also explicitly reasons about occlusions due to 3D object geometries for associating point tracks to individual objects.







